{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.6.0 (SDL 2.28.4, Python 3.11.5)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import random\n",
    "import pygame\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from pygame.math import Vector2\n",
    "import torch  # Often needed for policy_kwargs like activation_fn\n",
    "\n",
    "# Stable Baselines\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "from stable_baselines3.common.callbacks import CheckpointCallback, BaseCallback\n",
    "\n",
    "# ---------------- Constants and Utilities ----------------\n",
    "DEFAULT_SCREEN_WIDTH = 800    # simulation area width\n",
    "DEFAULT_SCREEN_HEIGHT = 600\n",
    "DEFAULT_MAX_STEPS = 2000\n",
    "\n",
    "# Sensor settings (not used in the simplified observation, but kept for simulation logic)\n",
    "NUM_SENSORS = 8\n",
    "SENSOR_STEP_SIZE = 5.0  # stepping distance for pseudo-ray\n",
    "\n",
    "# Colors\n",
    "WHITE       = (255, 255, 255)\n",
    "BLACK       = (0, 0, 0)\n",
    "GRAY        = (128, 128, 128)\n",
    "BLUE        = (0, 0, 255)\n",
    "YELLOW      = (255, 255, 0)\n",
    "DARK_RED    = (100, 0, 0)\n",
    "DARK_GREEN  = (0, 100, 0)\n",
    "CRASH_COLOR = (255, 100, 0)\n",
    "GREENISH    = (0, 200, 0)\n",
    "\n",
    "SENSOR_OBS_COLOR  = (50, 50, 50)\n",
    "\n",
    "EPSILON = 1e-6\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n",
    "\n",
    "class CustomLSTMExtractor(BaseFeaturesExtractor):\n",
    "    \"\"\"\n",
    "    Custom feature extractor using an LSTM.\n",
    "    This network processes the flat observation, applies a fully connected layer,\n",
    "    and then passes the result through an LSTM layer.\n",
    "    \n",
    "    Parameters:\n",
    "      observation_space: The gym observation space.\n",
    "      lstm_hidden_size: Number of hidden units in the LSTM (controls its size).\n",
    "    \"\"\"\n",
    "    def __init__(self, observation_space, lstm_hidden_size=128):\n",
    "        # We set features_dim equal to lstm_hidden_size so that the extracted\n",
    "        # feature vector has this dimension.\n",
    "        super(CustomLSTMExtractor, self).__init__(observation_space, features_dim=lstm_hidden_size)\n",
    "        self.lstm_hidden_size = lstm_hidden_size\n",
    "        input_dim = observation_space.shape[0]\n",
    "        \n",
    "        # A fully connected layer before the LSTM can help to scale the observation.\n",
    "        self.fc = nn.Linear(input_dim, lstm_hidden_size)\n",
    "        # LSTM layer: here num_layers=1 and batch_first=True.\n",
    "        self.lstm = nn.LSTM(lstm_hidden_size, lstm_hidden_size, batch_first=True)\n",
    "\n",
    "    def forward(self, observations):\n",
    "        # observations has shape: (batch_size, obs_dim)\n",
    "        x = F.relu(self.fc(observations))\n",
    "        # Add a time dimension (sequence length = 1) to use LSTM. New shape: (batch, 1, lstm_hidden_size)\n",
    "        x = x.unsqueeze(1)\n",
    "        # Process through LSTM; we discard the cell state.\n",
    "        x, (h_n, _) = self.lstm(x)\n",
    "        # h_n has shape: (num_layers, batch, lstm_hidden_size). Use the last layerâ€™s hidden state.\n",
    "        features = h_n[-1]  # shape: (batch, lstm_hidden_size)\n",
    "        return features\n",
    "\n",
    "\n",
    "\n",
    "def limit_vector(vector: Vector2, max_val: float) -> Vector2:\n",
    "    \"\"\"Clamp the magnitude of 'vector' to 'max_val'.\"\"\"\n",
    "    mag_sq = vector.magnitude_squared()\n",
    "    if mag_sq > max_val * max_val:\n",
    "        mag = math.sqrt(mag_sq) if mag_sq > EPSILON else EPSILON\n",
    "        vector = vector * (max_val / mag)\n",
    "    return vector\n",
    "\n",
    "# ---------------- Entity Classes ----------------\n",
    "class Obstacle:\n",
    "    def __init__(self, x, y, radius, screen_width, screen_height, max_speed=0.8):\n",
    "        self.position = Vector2(x, y)\n",
    "        self.radius = radius  # fixed size from parameter\n",
    "        self.screen_width = screen_width\n",
    "        self.screen_height = screen_height\n",
    "        angle = random.uniform(0, 2 * math.pi)\n",
    "        self.velocity = Vector2(math.cos(angle) * max_speed, math.sin(angle) * max_speed)\n",
    "        self.draw_radius = max(1, self.radius - 2)\n",
    "\n",
    "    def update(self):\n",
    "        self.position += self.velocity\n",
    "        if not (self.radius <= self.position.x <= self.screen_width - self.radius):\n",
    "            self.velocity.x *= -1\n",
    "            self.position.x = np.clip(self.position.x, self.radius, self.screen_width - self.radius)\n",
    "        if not (self.radius <= self.position.y <= self.screen_height - self.radius):\n",
    "            self.velocity.y *= -1\n",
    "            self.position.y = np.clip(self.position.y, self.radius, self.screen_height - self.radius)\n",
    "\n",
    "    def draw(self, surface):\n",
    "        pygame.draw.circle(surface, GRAY,\n",
    "                           (int(self.position.x), int(self.position.y)),\n",
    "                           int(self.draw_radius))\n",
    "\n",
    "class FastMovingTarget:\n",
    "    \"\"\"\n",
    "    A target that can optionally be made static and that responds to both drones\n",
    "    and obstacles using potential fields.\n",
    "    \"\"\"\n",
    "    def __init__(self, x, y, radius, screen_width, screen_height,\n",
    "                 max_speed=2.5, pf_params=None, avoidance_enabled=True, static_target=False):\n",
    "        self.position = Vector2(x, y)\n",
    "        self.initial_position = Vector2(x, y)  # Store initial position for observation\n",
    "        self.radius = radius\n",
    "        self.screen_width = screen_width\n",
    "        self.screen_height = screen_height\n",
    "        self.velocity = Vector2(max_speed, 0)\n",
    "        if self.velocity.length() < 0.1:\n",
    "            self.velocity = Vector2(max_speed / 2, 0)\n",
    "        self.max_speed = max_speed\n",
    "        self.pf = pf_params or {}\n",
    "        self.avoidance_enabled = avoidance_enabled\n",
    "        self.static_target = static_target\n",
    "        self.is_active = True  # Track if target is active\n",
    "\n",
    "    def _compute_potential_field_for_target(self, drones, obstacles):\n",
    "        net_force = Vector2(0, 0)\n",
    "        if not self.static_target:\n",
    "            rep_strength = self.pf.get(\"drone_repulsion_strength\", 100)\n",
    "            drone_influence_radius = self.pf.get(\"drone_influence_radius\", 50)\n",
    "            max_force = self.pf.get(\"max_drone_repulsion_force\", 15.0)\n",
    "            influence_radius_sq = drone_influence_radius ** 2\n",
    "            for d in drones:\n",
    "                vec_from_drone = self.position - d.position\n",
    "                dist_sq = vec_from_drone.magnitude_squared()\n",
    "                if EPSILON < dist_sq < influence_radius_sq:\n",
    "                    dist = math.sqrt(dist_sq)\n",
    "                    rep_magnitude = rep_strength * (1.0/dist - 1.0/drone_influence_radius) / dist\n",
    "                    force = vec_from_drone.normalize() * rep_magnitude\n",
    "                    net_force += limit_vector(force, max_force)\n",
    "        rep_strength_obs = self.pf.get(\"obs_repulsion_strength_for_target\", 3000)\n",
    "        obs_influence_radius = self.pf.get(\"obs_influence_radius_for_target\", 120)\n",
    "        max_force_obs = self.pf.get(\"max_obs_repulsion_force_for_target\", 25.0)\n",
    "        influence_radius_sq_obs = obs_influence_radius ** 2\n",
    "        for obs in obstacles:\n",
    "            vec_from_obs = self.position - obs.position\n",
    "            dist_sq = vec_from_obs.magnitude_squared()\n",
    "            if EPSILON < dist_sq < influence_radius_sq_obs:\n",
    "                dist = math.sqrt(dist_sq)\n",
    "                rep_magnitude = rep_strength_obs * (1.0/dist - 1.0/obs_influence_radius) / dist\n",
    "                force = vec_from_obs.normalize() * rep_magnitude\n",
    "                net_force += limit_vector(force, max_force_obs)\n",
    "        return net_force\n",
    "\n",
    "    def _compute_obstacle_repulsion_force(self, obstacles):\n",
    "        net_force = Vector2(0, 0)\n",
    "        rep_strength_obs = self.pf.get(\"obs_repulsion_strength_for_target\", 3000)\n",
    "        obs_influence_radius = self.pf.get(\"obs_influence_radius_for_target\", 120)\n",
    "        max_force_obs = self.pf.get(\"max_obs_repulsion_force_for_target\", 25.0)\n",
    "        influence_radius_sq_obs = obs_influence_radius ** 2\n",
    "        for obs in obstacles:\n",
    "            vec_from_obs = self.position - obs.position\n",
    "            dist_sq = vec_from_obs.magnitude_squared()\n",
    "            if EPSILON < dist_sq < influence_radius_sq_obs:\n",
    "                dist = math.sqrt(dist_sq)\n",
    "                rep_magnitude = rep_strength_obs * (1.0/dist - 1.0/obs_influence_radius) / dist\n",
    "                force = vec_from_obs.normalize() * rep_magnitude\n",
    "                net_force += limit_vector(force, max_force_obs)\n",
    "        return net_force\n",
    "\n",
    "    def update(self, drones, obstacles):\n",
    "        if not self.is_active or self.static_target:\n",
    "            return\n",
    "        if self.avoidance_enabled:\n",
    "            force = self._compute_potential_field_for_target(drones, obstacles)\n",
    "            self.velocity += force\n",
    "            self.velocity = limit_vector(self.velocity, self.max_speed)\n",
    "            self.position += self.velocity\n",
    "        else:\n",
    "            force = self._compute_obstacle_repulsion_force(obstacles)\n",
    "            self.velocity += force\n",
    "            self.velocity = limit_vector(self.velocity, self.max_speed)\n",
    "            self.position += self.velocity\n",
    "        target_wall_margin = max(self.radius, self.pf.get(\"target_wall_margin\", 500))\n",
    "        if not (target_wall_margin <= self.position.x <= self.screen_width - target_wall_margin):\n",
    "            self.velocity.x *= -1\n",
    "            self.position.x = np.clip(self.position.x, target_wall_margin, self.screen_width - target_wall_margin)\n",
    "        if not (target_wall_margin <= self.position.y <= self.screen_height - target_wall_margin):\n",
    "            self.velocity.y *= -1\n",
    "            self.position.y = np.clip(self.position.y, target_wall_margin, self.screen_height - target_wall_margin)\n",
    "\n",
    "    def draw(self, surface):\n",
    "        if self.is_active:\n",
    "            pygame.draw.circle(surface, GREENISH,\n",
    "                               (int(self.position.x), int(self.position.y)),\n",
    "                               self.radius)\n",
    "\n",
    "class Drone:\n",
    "    def __init__(self, x, y, drone_size=5, max_speed=3.0, screen_width=800, screen_height=600):\n",
    "        self.position = Vector2(x, y)\n",
    "        self.velocity = Vector2(max_speed, 0)\n",
    "        self.drone_size = drone_size\n",
    "        self.max_speed = max_speed\n",
    "        self.screen_width = screen_width\n",
    "        self.screen_height = screen_height\n",
    "        self.is_active = True  # Track if drone is active\n",
    "\n",
    "    def update(self, force: Vector2, damping=0.98):\n",
    "        if not self.is_active:\n",
    "            return\n",
    "        self.velocity += force\n",
    "        self.velocity *= damping\n",
    "        self.velocity = limit_vector(self.velocity, self.max_speed)\n",
    "        self.position += self.velocity\n",
    "\n",
    "    def draw(self, surface):\n",
    "        if self.is_active:\n",
    "            pygame.draw.circle(surface, BLUE,\n",
    "                           (int(self.position.x), int(self.position.y)),\n",
    "                           self.drone_size)\n",
    "\n",
    "# ---------- Helper: Draw Dashed Line ----------\n",
    "def draw_dashed_line(surface, color, start_pos, end_pos, dash_length=5, space_length=3):\n",
    "    start = np.array(start_pos)\n",
    "    end = np.array(end_pos)\n",
    "    line_vec = end - start\n",
    "    line_len = np.linalg.norm(line_vec)\n",
    "    if line_len == 0:\n",
    "        return\n",
    "    line_dir = line_vec / line_len\n",
    "    num_dashes = int(line_len // (dash_length + space_length))\n",
    "    for i in range(num_dashes + 1):\n",
    "        seg_start = start + (dash_length + space_length) * i * line_dir\n",
    "        seg_end = seg_start + dash_length * line_dir\n",
    "        if np.linalg.norm(seg_end - start) > line_len:\n",
    "            seg_end = end\n",
    "        pygame.draw.line(surface, color, seg_start, seg_end, 1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ---------------- DroneSwarmEnv ----------------\n",
    "class DroneSwarmEnv(gym.Env):\n",
    "    \"\"\"\n",
    "    Environment for controlling a swarm of drones via potential fields.\n",
    "    \"\"\"\n",
    "    metadata = {\"render_modes\": [\"human\", \"rgb_array\"], \"render_fps\": 100000}\n",
    "\n",
    "    def __init__(self,\n",
    "                 render_mode=\"human\",\n",
    "                 max_steps=DEFAULT_MAX_STEPS,\n",
    "                 screen_width=DEFAULT_SCREEN_WIDTH,\n",
    "                 screen_height=DEFAULT_SCREEN_HEIGHT,\n",
    "                 sensor_range=150,\n",
    "                 # Drones\n",
    "                 num_drones=5,\n",
    "                 drone_size=5,\n",
    "                 max_drone_speed=3.0,\n",
    "                 # Obstacles\n",
    "                 num_obstacles=5,\n",
    "                 obstacle_max_speed=0.8,\n",
    "                 obstacle_radius=20,\n",
    "                 # Targets\n",
    "                 num_targets=5,\n",
    "                 fast_target_max_speed=2.5,\n",
    "                 fast_target_radius=12,\n",
    "                 # Potential Field Hyperparameters\n",
    "                 pf_params=None,\n",
    "                 # Reward configuration\n",
    "                 reward_config=None,\n",
    "                 # Reward scaling configuration\n",
    "                 reward_ranges=None,\n",
    "                 # PF parameter bounding (defining min..max for BINARY action)\n",
    "                 controlled_pf_params=None,\n",
    "                 # Other parameters:\n",
    "                 enable_target_avoidance=True,\n",
    "                 drones_required_to_hit_target=2,\n",
    "                 max_drones_destroyed=None,\n",
    "                 intrinsic_reward_weight=0.1,\n",
    "                 num_static_targets=0,\n",
    "                 steps_required_to_hit_target=2,\n",
    "                 hit_distance_threshold=None):\n",
    "        super().__init__()\n",
    "        self.screen_width = screen_width\n",
    "        self.screen_height = screen_height\n",
    "        self.max_steps = max_steps\n",
    "        self.sensor_range = sensor_range\n",
    "        self.num_drones_init = num_drones\n",
    "        self.drone_size = drone_size\n",
    "        self.max_drone_speed = max_drone_speed\n",
    "        self.num_obstacles_init = num_obstacles\n",
    "        self.obstacle_max_speed = obstacle_max_speed\n",
    "        self.obstacle_radius = obstacle_radius\n",
    "        self.num_targets_init = num_targets\n",
    "        self.fast_target_max_speed = fast_target_max_speed\n",
    "        self.fast_target_radius = fast_target_radius\n",
    "        self.enable_target_avoidance = enable_target_avoidance\n",
    "        self.drones_required_to_hit_target = drones_required_to_hit_target\n",
    "        self.max_drones_destroyed = max_drones_destroyed\n",
    "        self.intrinsic_reward_weight = intrinsic_reward_weight\n",
    "        self.num_static_targets = num_static_targets\n",
    "        self.steps_required_to_hit_target = steps_required_to_hit_target\n",
    "        self.hit_distance_threshold = hit_distance_threshold if hit_distance_threshold is not None else (self.drone_size + self.fast_target_radius)\n",
    "\n",
    "        default_pf = {\n",
    "            \"attraction_strength\": 0.0005, \"max_attraction_force\": 0.8,\n",
    "            \"obs_repulsion_strength\": 7000, \"obs_influence_radius\": 100, \"max_obs_repulsion_force\": 20.0,\n",
    "            \"drone_repulsion_strength\": 100, \"drone_influence_radius\": 50, \"max_drone_repulsion_force\": 15.0,\n",
    "            \"boundary_repulsion_strength\": 6000, \"boundary_influence\": 40, \"max_boundary_force\": 18.0,\n",
    "            \"obs_repulsion_strength_for_target\": 3000, \"obs_influence_radius_for_target\": 120, \"max_obs_repulsion_force_for_target\": 25.0,\n",
    "            \"target_wall_margin\": 50\n",
    "        }\n",
    "        if pf_params is not None:\n",
    "            default_pf.update(pf_params)\n",
    "        self.pf = default_pf\n",
    "\n",
    "        default_reward_config = {\n",
    "            \"step_penalty\": 0.0,\n",
    "            \"hit_reward\": 100.0,\n",
    "            \"hitting_reward\": 10.0,  # New reward value for acting in a hitting manner\n",
    "            \"crash_penalty\": 50.0,\n",
    "            \"all_drones_lost_penalty\": 200.0,\n",
    "            \"all_targets_hit_reward\": 200.0,\n",
    "            \"timeout_penalty\": 50.0,\n",
    "            \"max_drones_destroyed_penalty\": -500.0\n",
    "        }\n",
    "\n",
    "        if reward_config is not None:\n",
    "            default_reward_config.update(reward_config)\n",
    "        self.reward_config = default_reward_config\n",
    "\n",
    "        default_reward_ranges = {\n",
    "            \"step_penalty\": {\"raw\": (-10.0, 0.0), \"scaled\": (-0.1, 0.0)},\n",
    "            \"hit_reward\": {\"raw\": (0.0, 300.0), \"scaled\": (0.0, 10.0)},\n",
    "            \"crash_penalty\": {\"raw\": (-150.0, 0.0), \"scaled\": (-10.0, 0.0)},\n",
    "            \"all_drones_lost\": {\"raw\": (-500.0, 0.0), \"scaled\": (-20.0, 0.0)},\n",
    "            \"all_targets_hit\": {\"raw\": (0.0, 300.0), \"scaled\": (0.0, 50.0)},\n",
    "            \"timeout\": {\"raw\": (-500.0, 0.0), \"scaled\": (-10, 0.0)},\n",
    "            \"max_drones_destroyed\": {\"raw\": (-500.0, 0.0), \"scaled\": (-20.0, 0.0)},\n",
    "            \"intrinsic_reward\": {\"raw\": (0.0, 1.0), \"scaled\": (0.0, 0.1)},\n",
    "\n",
    "            \"hitting_reward\": {\"raw\": (0.0, 20.0), \"scaled\": (0.0, 0.1)},  # New range for hitting reward\n",
    "\n",
    "        }\n",
    "\n",
    "        self.reward_ranges = reward_ranges if reward_ranges is not None else default_reward_ranges\n",
    "\n",
    "        if controlled_pf_params is None:\n",
    "            self.controlled_pf_params = {\n",
    "                \"attraction_strength\": {\"min\": 0.0001, \"max\": 0.01},\n",
    "                \"obs_repulsion_strength\": {\"min\": 500.0, \"max\": 50000.0},\n",
    "            }\n",
    "        else:\n",
    "            self.controlled_pf_params = controlled_pf_params\n",
    "        self.controlled_pf_keys = list(self.controlled_pf_params.keys())\n",
    "\n",
    "        # Action space remains unchanged.\n",
    "        action_dim = 2  # Only control x and y\n",
    "        low  = np.zeros(action_dim, dtype=np.float32)\n",
    "        high = np.ones(action_dim, dtype=np.float32)\n",
    "        self.action_space = gym.spaces.Box(low=low, high=high, shape=(action_dim,), dtype=np.float32)\n",
    "        \n",
    "        # Simplified Observation:\n",
    "        # - For each drone: 2 values (normalized x and y if active; (-1,-1) if inactive)\n",
    "        # - 5 obstacles: 5 * 2 = 10 values (normalized positions if detected; (-1,-1) if not)\n",
    "        # - 3 targets: 3 * 2 = 6 values (normalized positions if detected, (0,0) if hit, (-1,-1) if not detected)\n",
    "\n",
    "        # Original:\n",
    "        # obs_dim = (self.num_drones_init * (2 + NUM_SENSORS)) + 6\n",
    "\n",
    "        # Updated: now each drone contributes (2 positions + NUM_SENSORS sensor readings + 1 hitting flag)\n",
    "        obs_dim = (self.num_drones_init * (3 + NUM_SENSORS)) + 6\n",
    "        self.observation_space = gym.spaces.Box(\n",
    "            low=-1.0, high=1.0,\n",
    "            shape=(obs_dim,), dtype=np.float32\n",
    "        )\n",
    "\n",
    "        self.current_step = 0\n",
    "        self.render_mode = render_mode\n",
    "        self.screen = None\n",
    "        self.clock = None\n",
    "        self.info_font = None\n",
    "\n",
    "        self.drones = []\n",
    "        self.obstacles = []\n",
    "        self.fast_targets = []\n",
    "        self.target_pos = Vector2(self.screen_width * 0.5, self.screen_height * 0.5)\n",
    "\n",
    "        self.total_crashes_episode = 0\n",
    "        self.total_hits_episode = 0\n",
    "        self.current_episode_reward = 0.0\n",
    "        self.last_raw_action = None\n",
    "        self.last_extrinsic_reward = 0.0\n",
    "        self.last_intrinsic_reward = 0.0\n",
    "        self.last_step_reward = 0.0\n",
    "        self.last_reward_breakdown = {}\n",
    "        self.visited_cells = {}\n",
    "        self.target_hit_steps = {}\n",
    "        self.episode_num = 0\n",
    "\n",
    "        self.sidebar_width = 350\n",
    "        self.total_width = self.screen_width + self.sidebar_width\n",
    "        self.sidebar_scroll_offset = 0\n",
    "        self.sidebar_hscroll_offset = 0\n",
    "        self.slider_active = False\n",
    "        self.hslider_active = False\n",
    "\n",
    "        if self.render_mode == \"human\":\n",
    "            self._initialize_pygame()\n",
    "\n",
    "    def _initialize_pygame(self):\n",
    "        if self.screen is None:\n",
    "            pygame.init()\n",
    "            pygame.font.init()\n",
    "            self.screen = pygame.display.set_mode((self.total_width, self.screen_height))\n",
    "            pygame.display.set_caption(\"DroneSwarmEnv_lstm\")\n",
    "            self.clock = pygame.time.Clock()\n",
    "            try:\n",
    "                self.info_font = pygame.font.SysFont('Arial', 16)\n",
    "            except Exception:\n",
    "                self.info_font = pygame.font.SysFont(None, 16)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def compute_intrinsic_reward(self):\n",
    "        avg_pos = Vector2(0, 0)\n",
    "        active_drones = [d for d in self.drones if d.is_active]\n",
    "        num_drones = len(active_drones)\n",
    "        if num_drones == 0:\n",
    "            return 0.0\n",
    "        for d in active_drones:\n",
    "            avg_pos += d.position\n",
    "        avg_pos /= num_drones\n",
    "        grid_size = 50\n",
    "        cell = (int(avg_pos.x // grid_size), int(avg_pos.y // grid_size))\n",
    "        self.visited_cells[cell] = self.visited_cells.get(cell, 0) + 1\n",
    "        intrinsic_reward = 1.0 / self.visited_cells[cell]\n",
    "        return intrinsic_reward\n",
    "\n",
    "    def scale_reward(self, term_name, raw_value):\n",
    "        config = self.reward_ranges.get(term_name)\n",
    "        if config is None:\n",
    "            return raw_value\n",
    "        raw_min, raw_max = config[\"raw\"]\n",
    "        scaled_min, scaled_max = config[\"scaled\"]\n",
    "        if raw_max - raw_min == 0:\n",
    "            return scaled_min if raw_value <= raw_min else scaled_max\n",
    "        clipped = np.clip(raw_value, raw_min, raw_max)\n",
    "        ratio = (clipped - raw_min) / (raw_max - raw_min)\n",
    "        return scaled_min + ratio * (scaled_max - scaled_min)\n",
    "\n",
    "    def compute_reward(self, step_hits, step_crashes, all_drones_lost, all_targets_hit, timed_out):\n",
    "        step_penalty_raw    = self.reward_config.get(\"step_penalty\", 0.0)\n",
    "        hit_reward_raw      = step_hits * self.reward_config.get(\"hit_reward\", 100.0)\n",
    "        crash_penalty_raw   = step_crashes * self.reward_config.get(\"crash_penalty\", 50.0)\n",
    "        all_drones_lost_raw = self.reward_config.get(\"all_drones_lost_penalty\", 200.0) if all_drones_lost else 0.0\n",
    "        all_targets_hit_raw = self.reward_config.get(\"all_targets_hit_reward\", 200.0) if all_targets_hit else 0.0\n",
    "        timeout_raw         = self.reward_config.get(\"timeout_penalty\", 50.0) if timed_out else 0.0\n",
    "\n",
    "        max_drones_penalty_raw = 0.0\n",
    "        num_drones_active = sum(1 for d in self.drones if d.is_active)\n",
    "        num_drones_lost = self.num_drones_init - num_drones_active\n",
    "        if self.max_drones_destroyed is not None and num_drones_lost >= self.max_drones_destroyed:\n",
    "            max_drones_penalty_raw = self.reward_config.get(\"max_drones_destroyed_penalty\", -500.0)\n",
    "\n",
    "        # Compute the new \"hitting reward\" raw value.\n",
    "        # For each active drone, if it is close enough to any active target, add 1.\n",
    "        hitting_reward_raw = 0.0\n",
    "        for d in self.drones:\n",
    "            if d.is_active:\n",
    "                for ft in self.fast_targets:\n",
    "                    if ft.is_active and d.position.distance_to(ft.position) < self.hit_distance_threshold:\n",
    "                        hitting_reward_raw += 1\n",
    "                        # No break here, so a single drone can contribute multiple hits.\n",
    "\n",
    "        # Scale each reward term.\n",
    "        step_penalty = self.scale_reward(\"step_penalty\", step_penalty_raw)\n",
    "        hit_reward   = self.scale_reward(\"hit_reward\", hit_reward_raw)\n",
    "        # Scale the new hitting reward term.\n",
    "        hitting_reward = self.scale_reward(\"hitting_reward\", hitting_reward_raw)\n",
    "        crash_penalty = self.scale_reward(\"crash_penalty\", crash_penalty_raw)\n",
    "        all_drones_term = self.scale_reward(\"all_drones_lost\", all_drones_lost_raw)\n",
    "        all_targets_term = self.scale_reward(\"all_targets_hit\", all_targets_hit_raw)\n",
    "        timeout_term = self.scale_reward(\"timeout\", timeout_raw)\n",
    "        max_drones_term = self.scale_reward(\"max_drones_destroyed\", max_drones_penalty_raw)\n",
    "\n",
    "        # Include the new hitting reward in the extrinsic reward.\n",
    "        extrinsic = (step_penalty + hit_reward + hitting_reward + crash_penalty +\n",
    "                    all_drones_term + all_targets_term + timeout_term + max_drones_term)\n",
    "\n",
    "        intrinsic_raw = self.compute_intrinsic_reward() * self.intrinsic_reward_weight\n",
    "        intrinsic = self.scale_reward(\"intrinsic_reward\", intrinsic_raw)\n",
    "\n",
    "        total_reward = extrinsic + intrinsic\n",
    "\n",
    "        reward_breakdown = {\n",
    "            \"step_penalty\": step_penalty,\n",
    "            \"hit_reward\": hit_reward,\n",
    "            \"hitting_reward\": hitting_reward,  # New breakdown term.\n",
    "            \"crash_penalty\": crash_penalty,\n",
    "            \"all_drones_lost\": all_drones_term,\n",
    "            \"max_drones_destroyed\": max_drones_term,\n",
    "            \"all_targets_hit\": all_targets_term,\n",
    "            \"timeout\": timeout_term,\n",
    "            \"extrinsic\": extrinsic,\n",
    "            \"intrinsic\": intrinsic,\n",
    "            \"total\": total_reward\n",
    "        }\n",
    "        return reward_breakdown\n",
    "\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "        self.episode_num += 1\n",
    "        self.current_step = 0\n",
    "        self.total_crashes_episode = 0\n",
    "        self.total_hits_episode = 0\n",
    "        self.current_episode_reward = 0.0\n",
    "        self.last_raw_action = None\n",
    "        self.visited_cells = {}\n",
    "        self.target_hit_steps = {}\n",
    "\n",
    "        rng = self.np_random\n",
    "\n",
    "        self.drones = []\n",
    "        spawn_edge = rng.choice([\"left\", \"right\", \"top\", \"bottom\"])\n",
    "        for i in range(self.num_drones_init):\n",
    "            if spawn_edge == \"left\":\n",
    "                x = self.drone_size\n",
    "                y = rng.uniform(self.drone_size, self.screen_height - self.drone_size)\n",
    "            elif spawn_edge == \"right\":\n",
    "                x = self.screen_width - self.drone_size\n",
    "                y = rng.uniform(self.drone_size, self.screen_height - self.drone_size)\n",
    "            elif spawn_edge == \"top\":\n",
    "                x = rng.uniform(self.drone_size, self.screen_width - self.drone_size)\n",
    "                y = self.drone_size\n",
    "            else:\n",
    "                x = rng.uniform(self.drone_size, self.screen_width - self.drone_size)\n",
    "                y = self.screen_height - self.drone_size\n",
    "            d = Drone(x, y, drone_size=self.drone_size, max_speed=self.max_drone_speed,\n",
    "                      screen_width=self.screen_width, screen_height=self.screen_height)\n",
    "            self.drones.append(d)\n",
    "\n",
    "        self.obstacles = []\n",
    "        for _ in range(self.num_obstacles_init):\n",
    "            ox = rng.uniform(self.screen_width * 0.15, self.screen_width * 0.85)\n",
    "            oy = rng.uniform(self.screen_height * 0.15, self.screen_height * 0.85)\n",
    "            obs_obj = Obstacle(ox, oy, self.obstacle_radius, self.screen_width, self.screen_height,\n",
    "                           max_speed=self.obstacle_max_speed)\n",
    "            self.obstacles.append(obs_obj)\n",
    "\n",
    "        self.fast_targets = []\n",
    "        margin = self.pf.get(\"target_wall_margin\", 50)\n",
    "        attempts = 0\n",
    "        needed = self.num_targets_init\n",
    "        min_dist_from_obs = 15\n",
    "        min_dist_from_target = self.fast_target_radius * 3\n",
    "\n",
    "        while len(self.fast_targets) < needed and attempts < needed * 30:\n",
    "            attempts += 1\n",
    "            tx = rng.uniform(margin, self.screen_width - margin)\n",
    "            ty = rng.uniform(margin, self.screen_height - margin)\n",
    "            new_pos = Vector2(tx, ty)\n",
    "            valid_spawn = True\n",
    "            for obs_obj in self.obstacles:\n",
    "                if new_pos.distance_to(obs_obj.position) < (obs_obj.radius + self.fast_target_radius + min_dist_from_obs):\n",
    "                    valid_spawn = False\n",
    "                    break\n",
    "            if not valid_spawn:\n",
    "                continue\n",
    "            for existing_ft in self.fast_targets:\n",
    "                if new_pos.distance_to(existing_ft.position) < min_dist_from_target:\n",
    "                    valid_spawn = False\n",
    "                    break\n",
    "            if not valid_spawn:\n",
    "                continue\n",
    "            for d in self.drones:\n",
    "                if new_pos.distance_to(d.position) < (self.fast_target_radius + self.drone_size + 50):\n",
    "                    valid_spawn = False\n",
    "                    break\n",
    "            if not valid_spawn:\n",
    "                continue\n",
    "\n",
    "            is_static = len(self.fast_targets) < self.num_static_targets\n",
    "            ft = FastMovingTarget(\n",
    "                tx, ty, self.fast_target_radius, self.screen_width, self.screen_height,\n",
    "                max_speed=self.fast_target_max_speed, pf_params=self.pf,\n",
    "                avoidance_enabled=self.enable_target_avoidance, static_target=is_static)\n",
    "            ft.is_active = True\n",
    "            self.fast_targets.append(ft)\n",
    "            self.target_hit_steps[id(ft)] = 0\n",
    "\n",
    "        if len(self.fast_targets) < needed:\n",
    "            print(f\"Warning: Could only spawn {len(self.fast_targets)}/{needed} targets. Consider adjusting spawn parameters.\")\n",
    "\n",
    "        self.target_pos = Vector2(self.screen_width * 0.5, self.screen_height * 0.5)\n",
    "\n",
    "        obs = self._get_obs()\n",
    "        info = self._get_info()\n",
    "        if self.render_mode == \"human\":\n",
    "            self._render_frame()\n",
    "        return obs, info\n",
    "\n",
    "    def step(self, action):\n",
    "        self.current_step += 1\n",
    "        self.last_raw_action = action.copy()\n",
    "\n",
    "        tx = action[0] * self.screen_width\n",
    "        ty = action[1] * self.screen_height\n",
    "        self.target_pos = Vector2(tx, ty)\n",
    "\n",
    "        forces = [self._compute_potential_field_for_drone(d, i) for i, d in enumerate(self.drones) if d.is_active]\n",
    "        active_drone_indices = [i for i, d in enumerate(self.drones) if d.is_active]\n",
    "        for i, force in zip(active_drone_indices, forces):\n",
    "            self.drones[i].update(force)\n",
    "\n",
    "        for obs_obj in self.obstacles:\n",
    "            obs_obj.update()\n",
    "\n",
    "        active_drones_list = [d for d in self.drones if d.is_active]\n",
    "        for ft in self.fast_targets:\n",
    "            if ft.is_active:\n",
    "                ft.update(active_drones_list, self.obstacles)\n",
    "\n",
    "        step_crashes = 0\n",
    "        active_drone_indices_after_crash_check = []\n",
    "        if self.current_step < 100:\n",
    "            active_drone_indices_after_crash_check = [i for i, d in enumerate(self.drones) if d.is_active]\n",
    "        else:\n",
    "            for i, d in enumerate(self.drones):\n",
    "                if not d.is_active:\n",
    "                    continue\n",
    "                crashed = False\n",
    "                if not (self.drone_size <= d.position.x <= self.screen_width - self.drone_size and\n",
    "                        self.drone_size <= d.position.y <= self.screen_height - self.drone_size):\n",
    "                    crashed = True\n",
    "                if not crashed:\n",
    "                    for obs_obj in self.obstacles:\n",
    "                        if d.position.distance_squared_to(obs_obj.position) < (obs_obj.radius + d.drone_size) ** 2:\n",
    "                            crashed = True\n",
    "                            break\n",
    "                if not crashed:\n",
    "                    for j, other in enumerate(self.drones):\n",
    "                        if i == j or not other.is_active:\n",
    "                            continue\n",
    "                        if d.position.distance_squared_to(other.position) < (d.drone_size + other.drone_size - 1) ** 2:\n",
    "                            crashed = True\n",
    "                            break\n",
    "                if crashed:\n",
    "                    d.is_active = False\n",
    "                    step_crashes += 1\n",
    "                else:\n",
    "                    active_drone_indices_after_crash_check.append(i)\n",
    "        self.total_crashes_episode += step_crashes\n",
    "        num_drones_active = sum(1 for d in self.drones if d.is_active)\n",
    "\n",
    "        step_hits = 0\n",
    "        if num_drones_active > 0:\n",
    "            active_drones_list = [d for d in self.drones if d.is_active]\n",
    "            hit_target_ids_this_step = set()\n",
    "            for ft in self.fast_targets:\n",
    "                if not ft.is_active:\n",
    "                    continue\n",
    "                target_id = id(ft)\n",
    "                drones_near_target = 0\n",
    "                for d in active_drones_list:\n",
    "                    if d.position.distance_to(ft.position) < self.hit_distance_threshold:\n",
    "                        drones_near_target += 1\n",
    "                if drones_near_target >= self.drones_required_to_hit_target:\n",
    "                    self.target_hit_steps[target_id] = self.target_hit_steps.get(target_id, 0) + 1\n",
    "                else:\n",
    "                    self.target_hit_steps[target_id] = 0\n",
    "                if self.target_hit_steps[target_id] >= self.steps_required_to_hit_target:\n",
    "                    hit_target_ids_this_step.add(target_id)\n",
    "            if hit_target_ids_this_step:\n",
    "                for ft in self.fast_targets:\n",
    "                    if id(ft) in hit_target_ids_this_step and ft.is_active:\n",
    "                        ft.is_active = False\n",
    "                        step_hits += 1\n",
    "        self.total_hits_episode += step_hits\n",
    "        num_targets_active = sum(1 for ft in self.fast_targets if ft.is_active)\n",
    "\n",
    "        all_drones_lost = (num_drones_active == 0 and self.num_drones_init > 0)\n",
    "        all_targets_hit = (num_targets_active == 0 and self.num_targets_init > 0)\n",
    "\n",
    "        drones_destroyed_threshold_reached = False\n",
    "        if self.max_drones_destroyed is not None:\n",
    "            num_drones_lost = self.num_drones_init - num_drones_active\n",
    "            drones_destroyed_threshold_reached = (num_drones_lost >= self.max_drones_destroyed)\n",
    "\n",
    "        timed_out = (self.current_step >= self.max_steps)\n",
    "        terminated = all_drones_lost or all_targets_hit or drones_destroyed_threshold_reached\n",
    "        truncated = (timed_out and not terminated)\n",
    "\n",
    "        reward_breakdown = self.compute_reward(\n",
    "            step_hits, step_crashes, all_drones_lost, all_targets_hit, timed_out\n",
    "        )\n",
    "        reward = reward_breakdown[\"total\"]\n",
    "        self.last_reward_breakdown = reward_breakdown\n",
    "        self.last_extrinsic_reward = reward_breakdown[\"extrinsic\"]\n",
    "        self.last_intrinsic_reward = reward_breakdown[\"intrinsic\"]\n",
    "        self.last_step_reward = reward\n",
    "        self.current_episode_reward += reward\n",
    "\n",
    "        obs = self._get_obs()\n",
    "        info = self._get_info()\n",
    "\n",
    "        if self.render_mode == \"human\":\n",
    "            self._render_frame()\n",
    "\n",
    "        if np.isnan(obs).any() or np.isinf(obs).any():\n",
    "            print(\"Warning: NaN or Inf detected in observation!\")\n",
    "            obs = np.nan_to_num(obs, nan=0.0, posinf=1.0, neginf=-1.0)\n",
    "        if np.isnan(reward) or np.isinf(reward):\n",
    "            print(\"Warning: NaN or Inf detected in reward!\")\n",
    "            reward = 0.0\n",
    "\n",
    "        return obs, reward, terminated, truncated, info\n",
    "\n",
    "    def _compute_potential_field_for_drone(self, drone, drone_idx):\n",
    "        pf = self.pf\n",
    "        net_force = Vector2(0, 0)\n",
    "        attraction_strength = pf.get(\"attraction_strength\", 0.0005)\n",
    "        max_attraction_force = pf.get(\"max_attraction_force\", 0.8)\n",
    "        vec_to_target = self.target_pos - drone.position\n",
    "        dist_to_target = vec_to_target.length()\n",
    "        if dist_to_target > EPSILON:\n",
    "            force = vec_to_target.normalize() * attraction_strength * dist_to_target\n",
    "            net_force += limit_vector(force, max_attraction_force)\n",
    "        obs_repulsion_strength = pf.get(\"obs_repulsion_strength\", 7000)\n",
    "        obs_influence_radius = pf.get(\"obs_influence_radius\", 100)\n",
    "        max_obs_force = pf.get(\"max_obs_repulsion_force\", 20.0)\n",
    "        obs_infl_sq = obs_influence_radius ** 2\n",
    "        for obs_obj in self.obstacles:\n",
    "            vec_from_obs = drone.position - obs_obj.position\n",
    "            dist_sq = vec_from_obs.magnitude_squared()\n",
    "            if EPSILON < dist_sq < obs_infl_sq:\n",
    "                dist = math.sqrt(dist_sq)\n",
    "                if dist > EPSILON:\n",
    "                    rep_mag = obs_repulsion_strength * (1.0/dist - 1.0/obs_influence_radius) / dist\n",
    "                    force = vec_from_obs.normalize() * rep_mag\n",
    "                    net_force += limit_vector(force, max_obs_force)\n",
    "        drone_repulsion_strength = pf.get(\"drone_repulsion_strength\", 100)\n",
    "        drone_influence_radius = pf.get(\"drone_influence_radius\", 50)\n",
    "        max_drone_force = pf.get(\"max_drone_repulsion_force\", 15.0)\n",
    "        drone_infl_sq = drone_influence_radius ** 2\n",
    "        for i, other in enumerate(self.drones):\n",
    "            if i == drone_idx or not other.is_active:\n",
    "                continue\n",
    "            vec_from_other = drone.position - other.position\n",
    "            dist_sq = vec_from_other.magnitude_squared()\n",
    "            if EPSILON < dist_sq < drone_infl_sq:\n",
    "                dist = math.sqrt(dist_sq)\n",
    "                if dist > EPSILON:\n",
    "                    rep_mag = drone_repulsion_strength * (1.0/dist - 1.0/drone_influence_radius) / dist\n",
    "                    force = vec_from_other.normalize() * rep_mag\n",
    "                    net_force += limit_vector(force, max_drone_force)\n",
    "        boundary_repulsion_strength = pf.get(\"boundary_repulsion_strength\", 6000)\n",
    "        boundary_influence = pf.get(\"boundary_influence\", 40)\n",
    "        max_boundary_force = pf.get(\"max_boundary_force\", 18.0)\n",
    "        dist_x_left = drone.position.x\n",
    "        if dist_x_left < boundary_influence:\n",
    "            force_mag = boundary_repulsion_strength * (1.0 / max(dist_x_left, 1.0) - 1.0 / boundary_influence)\n",
    "            if force_mag > 0:\n",
    "                net_force += limit_vector(Vector2(1, 0) * force_mag, max_boundary_force)\n",
    "        dist_x_right = self.screen_width - drone.position.x\n",
    "        if dist_x_right < boundary_influence:\n",
    "            force_mag = boundary_repulsion_strength * (1.0 / max(dist_x_right, 1.0) - 1.0 / boundary_influence)\n",
    "            if force_mag > 0:\n",
    "                net_force += limit_vector(Vector2(-1, 0) * force_mag, max_boundary_force)\n",
    "        dist_y_top = drone.position.y\n",
    "        if dist_y_top < boundary_influence:\n",
    "            force_mag = boundary_repulsion_strength * (1.0 / max(dist_y_top, 1.0) - 1.0 / boundary_influence)\n",
    "            if force_mag > 0:\n",
    "                net_force += limit_vector(Vector2(0, 1) * force_mag, max_boundary_force)\n",
    "        dist_y_bottom = self.screen_height - drone.position.y\n",
    "        if dist_y_bottom < boundary_influence:\n",
    "            force_mag = boundary_repulsion_strength * (1.0 / max(dist_y_bottom, 1.0) - 1.0 / boundary_influence)\n",
    "            if force_mag > 0:\n",
    "                net_force += limit_vector(Vector2(0, -1) * force_mag, max_boundary_force)\n",
    "        return net_force\n",
    "\n",
    "\n",
    "    def _get_sensor_reading(self, drone: Drone, angle: float) -> float:\n",
    "        \"\"\"\n",
    "        Cast a ray from the drone's position at the given angle.\n",
    "        Return the normalized distance (distance/sensor_range) at which an obstacle or boundary is encountered.\n",
    "        If nothing is detected within sensor_range, return 1.0.\n",
    "        \"\"\"\n",
    "        direction = Vector2(math.cos(angle), math.sin(angle))\n",
    "        current_pos = Vector2(drone.position.x, drone.position.y)\n",
    "        distance = 0.0\n",
    "        while distance < self.sensor_range:\n",
    "            current_pos += direction * SENSOR_STEP_SIZE\n",
    "            distance += SENSOR_STEP_SIZE\n",
    "            # Check for boundary collision:\n",
    "            if (current_pos.x < 0 or current_pos.x > self.screen_width or\n",
    "                current_pos.y < 0 or current_pos.y > self.screen_height):\n",
    "                return distance / self.sensor_range\n",
    "            # Check for collision with any obstacle:\n",
    "            for obs_obj in self.obstacles:\n",
    "                if current_pos.distance_to(obs_obj.position) <= obs_obj.radius:\n",
    "                    return distance / self.sensor_range\n",
    "        return 1.0\n",
    "\n",
    "\n",
    "    def _get_obs(self):\n",
    "        obs = []\n",
    "        # For each drone, include normalized (x,y) position, sensor readings, and a hitting flag.\n",
    "        for d in self.drones:\n",
    "            if d.is_active:\n",
    "                pos_x = d.position.x / self.screen_width\n",
    "                pos_y = d.position.y / self.screen_height\n",
    "                obs.extend([pos_x, pos_y])\n",
    "                sensor_readings = []\n",
    "                for i in range(NUM_SENSORS):\n",
    "                    angle = (2 * math.pi * i) / NUM_SENSORS\n",
    "                    reading = self._get_sensor_reading(d, angle)\n",
    "                    sensor_readings.append(reading)\n",
    "                obs.extend(sensor_readings)\n",
    "                # Add a flag indicating if the drone is hitting a target.\n",
    "                hitting_count = 0.0\n",
    "                for ft in self.fast_targets:\n",
    "                    if ft.is_active and d.position.distance_to(ft.position) < self.hit_distance_threshold:\n",
    "                        hitting_count += 1.0\n",
    "                obs.append(hitting_count)\n",
    "            else:\n",
    "                # If inactive, fill with defaults (including the hitting flag).\n",
    "                obs.extend([-1.0, -1.0])\n",
    "                obs.extend([-1.0] * NUM_SENSORS)\n",
    "                obs.append(-1.0)\n",
    "        \n",
    "        # Targets: unchanged, still include up to 3 targets' positions.\n",
    "        target_detections = []\n",
    "        for ft in self.fast_targets:\n",
    "            if ft.is_active:\n",
    "                detected = False\n",
    "                min_distance = float('inf')\n",
    "                for d in self.drones:\n",
    "                    if d.is_active:\n",
    "                        dist = d.position.distance_to(ft.position)\n",
    "                        if dist <= self.sensor_range:\n",
    "                            detected = True\n",
    "                            if dist < min_distance:\n",
    "                                min_distance = dist\n",
    "                if detected:\n",
    "                    norm_x = ft.position.x / self.screen_width\n",
    "                    norm_y = ft.position.y / self.screen_height\n",
    "                    target_detections.append((min_distance, (norm_x, norm_y)))\n",
    "        target_detections.sort(key=lambda tup: tup[0])\n",
    "        count = 0\n",
    "        for _, pos in target_detections[:3]:\n",
    "            obs.extend(list(pos))\n",
    "            count += 1\n",
    "        for _ in range(3 - count):\n",
    "            obs.extend([-1.0, -1.0])\n",
    "            \n",
    "        return np.array(obs, dtype=np.float32)\n",
    "\n",
    "\n",
    "\n",
    "    def _get_info(self):\n",
    "        num_drones_active = sum(1 for d in self.drones if d.is_active)\n",
    "        num_targets_active = sum(1 for ft in self.fast_targets if ft.is_active)\n",
    "        return {\n",
    "            \"drones_active\": num_drones_active,\n",
    "            \"targets_active\": num_targets_active,\n",
    "            \"total_crashes\": self.total_crashes_episode,\n",
    "            \"total_hits\": self.total_hits_episode,\n",
    "            \"steps\": self.current_step,\n",
    "        }\n",
    "\n",
    "    def render(self):\n",
    "        if self.render_mode == \"human\":\n",
    "            if self.screen is None:\n",
    "                self._initialize_pygame()\n",
    "            self._render_frame()\n",
    "        elif self.render_mode == \"rgb_array\":\n",
    "            if self.screen is None:\n",
    "                self._initialize_pygame()\n",
    "            return self._render_frame()\n",
    "\n",
    "\n",
    "    def _build_sidebar_lines(self):\n",
    "        # Basic stats (episode, rewards, etc.)\n",
    "        num_drones_active = sum(1 for d in self.drones if d.is_active)\n",
    "        num_targets_active = sum(1 for ft in self.fast_targets if ft.is_active)\n",
    "        lines = [\n",
    "            f\"Episode: {self.episode_num}\",\n",
    "            f\"Ep Reward: {self.current_episode_reward:.2f}\",\n",
    "            f\"Crashes: {self.total_crashes_episode}\",\n",
    "            f\"Hits: {self.total_hits_episode}\",\n",
    "            (f\"Action: ({', '.join(f'{a:.2f}' for a in self.last_raw_action)})\"\n",
    "            if self.last_raw_action is not None else \"Action: (None yet)\"),\n",
    "            f\"Drones: {num_drones_active}/{self.num_drones_init}\",\n",
    "            f\"Targets: {num_targets_active}/{self.num_targets_init}\",\n",
    "            f\"Step: {self.current_step}/{self.max_steps}\",\n",
    "            f\"Step Reward: {self.last_step_reward:.2f}\",\n",
    "            \"--- Reward Breakdown ---\"\n",
    "        ]\n",
    "\n",
    "        if hasattr(self, 'last_reward_breakdown') and self.last_reward_breakdown:\n",
    "            for key, val in self.last_reward_breakdown.items():\n",
    "                lines.append(f\"  {key}: {val:.2f}\")\n",
    "        else:\n",
    "            lines.append(\"  (No breakdown yet)\")\n",
    "\n",
    "        lines.append(\"--- Controlled PF Params ---\")\n",
    "        if hasattr(self, 'controlled_pf_keys') and hasattr(self, 'pf'):\n",
    "            for key in self.controlled_pf_keys:\n",
    "                lines.append(f\"  {key}: {self.pf.get(key, 'N/A'):.4f}\")\n",
    "        else:\n",
    "            lines.append(\"  (PF Params not available)\")\n",
    "\n",
    "\n",
    "    \n",
    "        return lines\n",
    "\n",
    "\n",
    "    def _render_sidebar(self, lines):\n",
    "        sidebar_width = self.sidebar_width\n",
    "        sidebar_height = self.screen_height\n",
    "        sidebar_surface = pygame.Surface((sidebar_width, sidebar_height))\n",
    "        sidebar_surface.fill((50, 50, 50))  # Dark gray background\n",
    "\n",
    "        line_height = self.info_font.get_linesize()\n",
    "        v_spacing = 5\n",
    "        # Calculate the maximum number of lines that can be shown in the sidebar\n",
    "        max_lines = sidebar_height // (line_height + v_spacing)\n",
    "        \n",
    "        # Use the stored scroll offset to determine which lines to show.\n",
    "        start_index = self.sidebar_scroll_offset\n",
    "        visible_lines = lines[start_index:start_index + max_lines]\n",
    "        \n",
    "        y = v_spacing\n",
    "        for line in visible_lines:\n",
    "            try:\n",
    "                text_surf = self.info_font.render(line, True, WHITE)\n",
    "                sidebar_surface.blit(text_surf, (10, y))\n",
    "            except Exception as e:\n",
    "                error_surf = self.info_font.render(f\"Render error: {e}\", True, (255, 100, 100))\n",
    "                sidebar_surface.blit(error_surf, (10, y))\n",
    "            y += line_height + v_spacing\n",
    "\n",
    "        # If there are more lines than can be shown, draw a simple slider.\n",
    "        total_lines = len(lines)\n",
    "        if total_lines > max_lines:\n",
    "            # The slider height is proportional to the ratio of visible lines to total lines.\n",
    "            slider_height = max(20, sidebar_height * max_lines // total_lines)\n",
    "            # Compute slider vertical position based on scroll offset.\n",
    "            slider_y = (self.sidebar_scroll_offset / total_lines) * sidebar_height\n",
    "            slider_rect = pygame.Rect(sidebar_width - 15, slider_y, 10, slider_height)\n",
    "            pygame.draw.rect(sidebar_surface, GRAY, slider_rect)\n",
    "        \n",
    "        return sidebar_surface\n",
    "\n",
    "    def _render_frame(self):\n",
    "        # Process events for the sidebar scroll\n",
    "        for event in pygame.event.get():\n",
    "            if event.type == pygame.MOUSEBUTTONDOWN:\n",
    "                if event.button == 4:  # Scroll up\n",
    "                    self.sidebar_scroll_offset = max(0, self.sidebar_scroll_offset - 1)\n",
    "                elif event.button == 5:  # Scroll down\n",
    "                    self.sidebar_scroll_offset += 1\n",
    "        \n",
    "        # Build sidebar lines and render the sidebar as before\n",
    "        sidebar_lines = self._build_sidebar_lines()\n",
    "        sidebar_surface = self._render_sidebar(sidebar_lines)\n",
    "        sim_surface = self._render_simulation()\n",
    "        \n",
    "        self.screen.fill(BLACK)\n",
    "        self.screen.blit(sidebar_surface, (0, 0))\n",
    "        self.screen.blit(sim_surface, (self.sidebar_width, 0))\n",
    "        pygame.display.flip()\n",
    "\n",
    "        if self.clock:\n",
    "            self.clock.tick(self.metadata[\"render_fps\"])\n",
    "\n",
    "        if self.render_mode == \"rgb_array\":\n",
    "            try:\n",
    "                return np.transpose(pygame.surfarray.array3d(sim_surface), axes=(1, 0, 2))\n",
    "            except pygame.error as e:\n",
    "                print(f\"Error getting surface array: {e}\")\n",
    "                return np.zeros((self.screen_height, self.screen_width, 3), dtype=np.uint8)\n",
    "        return None\n",
    "\n",
    "    def _render_simulation(self):\n",
    "            sim_surface = pygame.Surface((self.screen_width, self.screen_height))\n",
    "            sim_surface.fill(BLACK)\n",
    "\n",
    "            # Draw obstacles, targets, etc.\n",
    "            for obs_obj in self.obstacles:\n",
    "                if hasattr(obs_obj, 'draw') and callable(obs_obj.draw):\n",
    "                    obs_obj.draw(sim_surface)\n",
    "\n",
    "            try:\n",
    "                target_pos_int = (int(self.target_pos.x), int(self.target_pos.y))\n",
    "                pygame.draw.circle(sim_surface, YELLOW, target_pos_int, 10)\n",
    "                pygame.draw.circle(sim_surface, WHITE, target_pos_int, 10, 2)\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "            for ft in self.fast_targets:\n",
    "                if ft.is_active and hasattr(ft, 'draw') and callable(ft.draw):\n",
    "                    ft.draw(sim_surface)\n",
    "                    if hasattr(self, 'target_hit_steps') and self.target_hit_steps.get(id(ft), 0) > 0:\n",
    "                        try:\n",
    "                            ft_pos_int = (int(ft.position.x), int(ft.position.y))\n",
    "                            radius_int = int(ft.radius + 3)\n",
    "                            pygame.draw.circle(sim_surface, DARK_RED, ft_pos_int, radius_int, 2)\n",
    "                        except Exception:\n",
    "                            pass\n",
    "                if hasattr(ft, 'initial_position'):\n",
    "                    try:\n",
    "                        init_pos_int = (int(ft.initial_position.x), int(ft.initial_position.y))\n",
    "                        pygame.draw.circle(sim_surface, DARK_GREEN, init_pos_int, 3, 1)\n",
    "                    except Exception:\n",
    "                        pass\n",
    "\n",
    "            # (Optional) Draw hit-line indicators\n",
    "            hit_threshold = getattr(self, 'hit_distance_threshold', float('inf'))\n",
    "            for d in self.drones:\n",
    "                if d.is_active:\n",
    "                    for ft in self.fast_targets:\n",
    "                        if ft.is_active and d.position.distance_to(ft.position) < hit_threshold:\n",
    "                            start_pos = (int(d.position.x), int(d.position.y))\n",
    "                            end_pos = (int(ft.position.x), int(ft.position.y))\n",
    "                            pygame.draw.line(sim_surface, GREENISH, start_pos, end_pos, 2)\n",
    "            \n",
    "            # ---------------- Render sensor rays ----------------\n",
    "            # Create a transparent surface for sensor lines\n",
    "            sensor_surface = pygame.Surface((self.screen_width, self.screen_height), pygame.SRCALPHA)\n",
    "            # Use a very faint white color (alpha 30) for subtle sensor lines\n",
    "            sensor_color = (255, 255, 255, 30)\n",
    "            for d in self.drones:\n",
    "                if d.is_active:\n",
    "                    for i in range(NUM_SENSORS):\n",
    "                        angle = (2 * math.pi * i) / NUM_SENSORS\n",
    "                        start_pos = (int(d.position.x), int(d.position.y))\n",
    "                        # Sensor endpoint using the environment sensor_range\n",
    "                        end_vector = Vector2(math.cos(angle), math.sin(angle)) * self.sensor_range\n",
    "                        end_pos = (int(d.position.x + end_vector.x), int(d.position.y + end_vector.y))\n",
    "                        draw_dashed_line(sensor_surface, sensor_color, start_pos, end_pos, dash_length=5, space_length=3)\n",
    "            # Blit the sensor layer on top\n",
    "            sim_surface.blit(sensor_surface, (0, 0))\n",
    "            # -------------------------------------------------------\n",
    "            \n",
    "            # Finally, draw the drones\n",
    "            for d in self.drones:\n",
    "                if d.is_active:\n",
    "                    d.draw(sim_surface)\n",
    "\n",
    "            return sim_surface\n",
    "\n",
    "    def close(self):\n",
    "        if self.screen is not None:\n",
    "            pygame.display.quit()\n",
    "            pygame.font.quit()\n",
    "            pygame.quit()\n",
    "            self.screen = None\n",
    "            self.clock = None\n",
    "            self.info_font = None\n",
    "\n",
    "# ---------------- Render Callback for Stable Baselines ----------------\n",
    "class RenderCallback(BaseCallback):\n",
    "    def __init__(self, render_freq=100, verbose=0):\n",
    "        super().__init__(verbose)\n",
    "        self.render_freq = render_freq\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        if self.n_calls % self.render_freq == 0:\n",
    "            self.training_env.envs[0].render()\n",
    "        return True\n",
    "\n",
    "# --------------- Example Tuning Parameters Section ---------------\n",
    "tuning_reward_ranges = {\n",
    "    \"step_penalty\": {\"raw\": (-10.0, 0.0), \"scaled\": (-0.01, 0.0)},\n",
    "    \"hit_reward\": {\"raw\": (0.0, 300.0), \"scaled\": (0.0, 10.0)},\n",
    "    \"crash_penalty\": {\"raw\": (-150.0, 0.0), \"scaled\": (-10.0, 0.0)},\n",
    "    \"all_drones_lost\": {\"raw\": (-500.0, 0.0), \"scaled\": (-0.0, 0.0)},\n",
    "    \"all_targets_hit\": {\"raw\": (0.0, 300.0), \"scaled\": (0.0, 50.0)},\n",
    "    \"timeout\": {\"raw\": (-500.0, 0.0), \"scaled\": (-10, 0.0)},\n",
    "    \"max_drones_destroyed\": {\"raw\": (-500.0, 0.0), \"scaled\": (-20.0, 0.0)},\n",
    "    \"intrinsic_reward\": {\"raw\": (0.0, 1.0), \"scaled\": (0.0, 0.1)},\n",
    "    \"hitting_reward\": {\"raw\": (0.0, 20.0), \"scaled\": (0.0, 0.1)},  # New range for hitting reward\n",
    "\n",
    "}\n",
    "\n",
    "# ---------------- Example of Custom Init Params ----------------\n",
    "train_env_params = {\n",
    "    \"render_mode\": \"\",\n",
    "    \"max_steps\": 3000,\n",
    "    \"screen_width\": 600,\n",
    "    \"screen_height\": 600,\n",
    "    \"num_drones\": 6,\n",
    "    \"drone_size\": 6,\n",
    "    \"max_drone_speed\": 7.0,\n",
    "    \"num_obstacles\":2,\n",
    "    \"obstacle_max_speed\":4,\n",
    "    \"obstacle_radius\": 25,\n",
    "    \"num_targets\": 10,\n",
    "    \"fast_target_max_speed\": 0.0,\n",
    "    \"fast_target_radius\": 10,\n",
    "    \"sensor_range\": 150,\n",
    "    \"pf_params\": {\n",
    "        \"attraction_strength\": 0.01,\n",
    "        \"max_attraction_force\": 1.0,\n",
    "        \"obs_repulsion_strength\": 3000,\n",
    "        \"obs_influence_radius\": 150,\n",
    "        \"max_obs_repulsion_force\": 10000.0,\n",
    "        \"drone_repulsion_strength\": 120,\n",
    "        \"drone_influence_radius\": 150,\n",
    "        \"max_drone_repulsion_force\": 18.0,\n",
    "        \"boundary_repulsion_strength\": 1000,\n",
    "        \"boundary_influence\": 10,\n",
    "        \"max_boundary_force\": 20.0,\n",
    "        \"obs_repulsion_strength_for_target\": 3000,\n",
    "        \"obs_influence_radius_for_target\": 120,\n",
    "        \"max_obs_repulsion_force_for_target\": 25.0,\n",
    "        \"target_wall_margin\": 50\n",
    "    },\n",
    "    \"reward_config\": {\n",
    "        \"step_penalty\": -0.01,\n",
    "        \"hit_reward\": 150.0,\n",
    "        \"crash_penalty\": -80.0,\n",
    "        \"all_drones_lost_penalty\": -500.0,\n",
    "        \"all_targets_hit_reward\": 300.0,\n",
    "        \"timeout_penalty\": -500.0,\n",
    "        \"max_drones_destroyed_penalty\": -500.0\n",
    "    },\n",
    "    \"reward_ranges\": tuning_reward_ranges,\n",
    "    \"controlled_pf_params\": {},\n",
    "    \"enable_target_avoidance\": False,\n",
    "    \"drones_required_to_hit_target\": 3,\n",
    "    \"steps_required_to_hit_target\": 1,\n",
    "    \"hit_distance_threshold\": 50,\n",
    "    \"max_drones_destroyed\": 4,\n",
    "    \"intrinsic_reward_weight\": 1,\n",
    "    \"num_static_targets\": 7\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting episode 1...\n",
      "Episode 1 finished. Info: {'drones_active': 2, 'targets_active': 7, 'total_crashes': 4, 'total_hits': 3, 'steps': 671}\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import time\n",
    "env = DroneSwarmEnv(**train_env_params)\n",
    "num_episodes = 1  # Number of episodes to test\n",
    "for ep in range(num_episodes):\n",
    "    obs, info = env.reset()\n",
    "    done = False\n",
    "    print(f\"Starting episode {ep + 1}...\")\n",
    "    while not done:\n",
    "        # Use random actions for testing.\n",
    "        # Action: [target_x, target_y, pf_attraction_control, pf_obs_repulsion_control]\n",
    "        action = env.action_space.sample()\n",
    "        obs, reward, done, truncated, info = env.step(action)\n",
    "        env.render()\n",
    "\n",
    "    print(f\"Episode {ep + 1} finished. Info: {info}\")\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using mps device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/an/anaconda3/lib/python3.11/site-packages/stable_baselines3/common/policies.py:486: UserWarning: As shared layers in the mlp_extractor are removed since SB3 v1.8.0, you should now pass directly a dictionary and not a list (net_arch=dict(pi=..., vf=...) instead of net_arch=[dict(pi=..., vf=...)])\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to ./tensorboard_logs/RecurrentPPO_1\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 931      |\n",
      "|    ep_rew_mean     | -10.3    |\n",
      "| time/              |          |\n",
      "|    fps             | 236      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 4        |\n",
      "|    total_timesteps | 1024     |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 608          |\n",
      "|    ep_rew_mean          | -19.3        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 129          |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 2048         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0069105774 |\n",
      "|    clip_fraction        | 0.112        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.84        |\n",
      "|    explained_variance   | 0.0239       |\n",
      "|    learning_rate        | 0.001        |\n",
      "|    loss                 | 0.0319       |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | -0.0048      |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 6.9          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 722          |\n",
      "|    ep_rew_mean          | -18.4        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 111          |\n",
      "|    iterations           | 3            |\n",
      "|    time_elapsed         | 27           |\n",
      "|    total_timesteps      | 3072         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0037770625 |\n",
      "|    clip_fraction        | 0.0989       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.84        |\n",
      "|    explained_variance   | 0.468        |\n",
      "|    learning_rate        | 0.001        |\n",
      "|    loss                 | 9.34         |\n",
      "|    n_updates            | 20           |\n",
      "|    policy_gradient_loss | -0.00344     |\n",
      "|    std                  | 0.999        |\n",
      "|    value_loss           | 9.34         |\n",
      "------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "env = DroneSwarmEnv(**train_env_params)\n",
    "\n",
    "log_dir = \"./tensorboard_logs/\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "checkpoint_callback = CheckpointCallback(save_freq=100000, \n",
    "                                         save_path='./checkpoints_lstm/', \n",
    "                                         name_prefix='ppo_drone_swarm')\n",
    "\n",
    "render_callback = RenderCallback(render_freq=1000)\n",
    "# default_policy_kwargs = dict(\n",
    "#     activation_fn=torch.nn.Tanh,\n",
    "#     net_arch=dict(\n",
    "#         pi=[ 256,256], \n",
    "#         vf=[ 256,256])\n",
    "# )\n",
    "# Set up policy keyword arguments with the custom extractor.\n",
    "# Import RecurrentPPO from sb3-contrib\n",
    "from sb3_contrib import RecurrentPPO\n",
    "\n",
    "# Define your LSTM policy parameters\n",
    "policy_kwargs = dict(\n",
    "    activation_fn=torch.nn.ELU,\n",
    "    net_arch=[dict(pi=[ 256, 256], vf=[256, 256])],\n",
    "    lstm_hidden_size=256,  # Set the hidden size of the LSTM layer\n",
    ")\n",
    "\n",
    "# Create the model using RecurrentPPO with the MlpLstmPolicy\n",
    "model = RecurrentPPO(\n",
    "    policy=\"MlpLstmPolicy\",  # Use a recurrent policy\n",
    "    env=env,\n",
    "    learning_rate=1e-3,\n",
    "    n_steps=1024,    # Ensure this value works with your recurrence settings\n",
    "    batch_size=64,   # This may need to be divisible by the recurrent sequence length\n",
    "    n_epochs=10,\n",
    "    gamma=0.99,\n",
    "    gae_lambda=0.95,\n",
    "    clip_range=0.2,\n",
    "    ent_coef=0.0,\n",
    "    vf_coef=0.5,\n",
    "    max_grad_norm=0.5,\n",
    "    tensorboard_log=log_dir,\n",
    "    policy_kwargs=policy_kwargs,\n",
    "    verbose=1,\n",
    "    seed=None,\n",
    "    device=\"mps\"\n",
    ")\n",
    "\n",
    "# Train the model as before\n",
    "total_timesteps = 10_000_000\n",
    "model.learn(total_timesteps=total_timesteps, callback=[checkpoint_callback, render_callback])\n",
    "\n",
    "model.save(\"ppo_drone_swarm_final\")\n",
    "print(\"Training complete. Model saved as 'ppo_drone_swarm_final'.\")\n",
    "env.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pygame\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.callbacks import CheckpointCallback, BaseCallback\n",
    "\n",
    "# If your DroneSwarmEnv and RenderCallback classes (and train_env_params) are in a separate file,\n",
    "# import them. For example:\n",
    "# from drone_swarm_env import DroneSwarmEnv, RenderCallback, train_env_params\n",
    "\n",
    "# Re-create the environment using the same parameters as before.\n",
    "env = DroneSwarmEnv(**train_env_params)\n",
    "\n",
    "# Setup callbacks (adjust the frequency or names if desired)\n",
    "checkpoint_callback = CheckpointCallback(\n",
    "    save_freq=2, \n",
    "    save_path='./checkpoints_lstm/', \n",
    "    name_prefix='ppo_drone_swarm_retrain'\n",
    ")\n",
    "render_callback = RenderCallback(render_freq=50)\n",
    "\n",
    "# Load the previously saved model and attach the environment.\n",
    "model = PPO.load(\"checkpoints/ppo_drone_swarm_378_steps.zip\", env=env)\n",
    "\n",
    "# Optionally, adjust additional training parameters here.\n",
    "additional_timesteps = 5_000_000  # for example, 5 million additional timesteps\n",
    "\n",
    "# Continue training (retraining) the model.\n",
    "model.learn(total_timesteps=additional_timesteps, \n",
    "            callback=[checkpoint_callback, \n",
    "                      render_callback])\n",
    "\n",
    "# Save the retrained model.\n",
    "model.save(\"ppo_drone_swarm_final_retrained_lstm\")\n",
    "print(\"Retraining complete. Model saved as 'ppo_drone_swarm_final_retrained_lstm'.\")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import gymnasium as gym\n",
    "from stable_baselines3 import PPO\n",
    "\n",
    "# Create an evaluation environment with human rendering.\n",
    "eval_env_params = train_env_params.copy()\n",
    "eval_env_params[\"render_mode\"] = \"human\"\n",
    "eval_env = DroneSwarmEnv(**eval_env_params)\n",
    "\n",
    "# Load the saved model and pass the evaluation environment.\n",
    "model = PPO.load(\"ppo_drone_swarm_final\", env=eval_env)\n",
    "\n",
    "num_episodes = 5\n",
    "for ep in range(num_episodes):\n",
    "    obs, info = eval_env.reset()\n",
    "    done = False\n",
    "    truncated = False\n",
    "    episode_reward = 0\n",
    "    while not (done or truncated):\n",
    "        # Predict action from the trained model.\n",
    "        action, _ = model.predict(obs, deterministic=True)\n",
    "        obs, reward, done, truncated, info = eval_env.step(action)\n",
    "        episode_reward += reward\n",
    "        eval_env.render()\n",
    "        time.sleep(0.02)  # Slow down rendering for visualization.\n",
    "    print(f\"Episode {ep+1} reward: {episode_reward}\")\n",
    "\n",
    "eval_env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
